{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fruit Ripeness Level Classification\n",
        "This notebook contains the Data Processing and Model training for the Ruit Ripeness Level Classification. Team Okra's Hackathon Solution."
      ],
      "metadata": {
        "id": "cRD8S4TJ4VDa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v9x4tpNKQ0z"
      },
      "source": [
        "# Datasets Download\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Secondary Datasets(Roboflow)"
      ],
      "metadata": {
        "id": "WEabDy3M5JVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-93FZ9AZ6-zp",
        "outputId": "bd643f18-a299-4235-f683-f77542a9dc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in banana-2 to yolov8:: 100%|██████████| 73845/73845 [00:07<00:00, 10112.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to banana-2 in yolov8:: 100%|██████████| 3858/3858 [00:00<00:00, 7113.45it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow -qq\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"***********\")\n",
        "project = rf.workspace(\"arm-oeppz\").project(\"banana-8qkur\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSSpkUaXDsCn",
        "outputId": "7f1f1276-4966-4d54-c0d2-600049e8a267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in rot_detection-13 to yolov8:: 100%|██████████| 173631/173631 [00:13<00:00, 13140.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to rot_detection-13 in yolov8:: 100%|██████████| 7972/7972 [00:01<00:00, 6582.63it/s]\n"
          ]
        }
      ],
      "source": [
        "project = rf.workspace(\"srmist-doq3j\").project(\"rot_detection\")\n",
        "version = project.version(13)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5E_in2r9hav",
        "outputId": "dc1c4a6e-f30d-4f3b-a833-9e89839dc626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in orange_detection-2 to yolov8:: 100%|██████████| 21727/21727 [00:02<00:00, 8082.20it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to orange_detection-2 in yolov8:: 100%|██████████| 1230/1230 [00:00<00:00, 9426.76it/s]\n"
          ]
        }
      ],
      "source": [
        "project = rf.workspace(\"mert6107\").project(\"orange_detection-5f84p\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-jmm7fVBjxI",
        "outputId": "17c10317-a065-4d18-d822-3b7dc9e4ffec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in tomato-checker-1 to yolov8:: 100%|██████████| 235668/235668 [00:14<00:00, 16347.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to tomato-checker-1 in yolov8:: 100%|██████████| 12660/12660 [00:02<00:00, 5767.22it/s]\n"
          ]
        }
      ],
      "source": [
        "project = rf.workspace(\"money-detection-xez0r\").project(\"tomato-checker\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primary Dataset (Copy from Drive)"
      ],
      "metadata": {
        "id": "AxBI0m6S5Q3o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqTwKUCYITHI"
      },
      "outputs": [],
      "source": [
        "# copy orange project dataset in drive to the Orange_Project\n",
        "!cp -r /content/drive/MyDrive/Orange_Project /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Datasets Notes\n",
        "Banana Ripeness Dataset: https://universe.roboflow.com/arm-oeppz/banana-8qkur/dataset/2\n",
        "folde_name = banana-2\n",
        "names:\n",
        "- ripe banana\n",
        "- riper banana\n",
        "- rotten banana\n",
        "- unripe banana\n",
        "\n",
        "Orange dataset: Google Drive\n",
        "Folder_name: Orange_Project\n",
        "classes:\n",
        "- Ripe\n",
        "- Unripe\n",
        "\n",
        "Orange dataset 2: https://universe.roboflow.com/mert6107/orange_detection-5f84p/dataset/\n",
        "folder_name: orange_detection-2\n",
        "classes:\n",
        "names:\n",
        "- RottenOranges\n",
        "- orange\n",
        "\n",
        "Tomato Dataset(use only unripe class):\n",
        "folder_name: tomato-checker-1\n",
        "classes:\n",
        "names:\n",
        "- damaged\n",
        "- healthy ripe\n",
        "- unripe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Rot detection dataset: https://universe.roboflow.com/srmist-doq3j/rot_detection/dataset/13\n",
        "Classes:\n",
        "names:\n",
        "- cucumber_healthy\n",
        "- cucumber_rotten\n",
        "- eggplant_healthy\n",
        "- eggplant_rotten\n",
        "- grapes_healthy\n",
        "- grapes_rotten\n",
        "- spinach_healthy\n",
        "- spinach_rotten\n",
        "- tomato_healthy\n",
        "- tomato_rotten"
      ],
      "metadata": {
        "id": "xlCY-Hz550aW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_HcF2pN8D1u"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI5hX5N-KWQc"
      },
      "source": [
        "# Data Processing\n",
        "This code combines multiple labeled fruit and vegetable image datasets into a unified dataset for training a model. The process involves:\n",
        "\n",
        "1. Class Mapping: Maps different dataset-specific class names (e.g., \"ripe banana\", \"cucumber_rotten\") to standard labels: ripe, unripe, rotten.\n",
        "\n",
        "2. Dataset Processing: Iterates over several datasets, reads image-label pairs, filters out unwanted classes, and limits each class to 3,000 samples.\n",
        "\n",
        "3. Label Conversion: Updates label files to use the new unified class IDs based on the mapping.\n",
        "\n",
        "4. Output Structure: Copies selected images and updated labels into a new combined_dataset directory, organized into train and val folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phYKUo_vwUZU",
        "outputId": "8d0ea2cf-ae46-4afc-c9ef-7b6e58c11dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before re-splitting:\n",
            "  ripe: 3000 images\n",
            "  rotten: 2879 images\n",
            "  unripe: 2105 images\n",
            "\n",
            "Final split statistics:\n",
            "Train:\n",
            "  ripe: 2100 images\n",
            "  rotten: 2015 images\n",
            "  unripe: 1473 images\n",
            "\n",
            "Validation:\n",
            "  ripe: 900 images\n",
            "  rotten: 864 images\n",
            "  unripe: 632 images\n",
            "\n",
            "Combined dataset created at: /content/combined_dataset\n",
            "Dataset YAML file: combined_dataset/dataset.yaml\n",
            "\n",
            "Class ID mapping:\n",
            "  0: ripe\n",
            "  1: rotten\n",
            "  2: unripe\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def create_directory(directory):\n",
        "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "def load_yaml(yaml_file):\n",
        "    \"\"\"Load YAML file\"\"\"\n",
        "    with open(yaml_file, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "def save_yaml(data, yaml_file):\n",
        "    \"\"\"Save data to YAML file\"\"\"\n",
        "    with open(yaml_file, 'w') as f:\n",
        "        yaml.dump(data, f, default_flow_style=False)\n",
        "\n",
        "def parse_class_mapping():\n",
        "    \"\"\"Map original dataset classes to our target classes\"\"\"\n",
        "    class_mapping = {\n",
        "        # Banana dataset mappings\n",
        "        'ripe banana': 'ripe',\n",
        "        'riper banana': 'ripe',  # Treat 'riper' as 'ripe'\n",
        "        'rotten banana': 'rotten',\n",
        "        'unripe banana': 'unripe',\n",
        "\n",
        "        # Orange dataset mappings\n",
        "        'Ripe': 'ripe',\n",
        "        'Unripe': 'unripe',\n",
        "        'RottenOranges': 'rotten',\n",
        "        'orange': 'ripe',  #  general 'orange' is ripe\n",
        "\n",
        "        # Tomato dataset mappings - only using unripe class as requested\n",
        "        'unripe': 'unripe',\n",
        "        'healthy ripe': 'ripe',\n",
        "        'damaged': None,  # Not using this class\n",
        "\n",
        "        # Rot detection dataset mappings\n",
        "        'cucumber_healthy': 'ripe',  #  'healthy' is 'ripe'\n",
        "        'cucumber_rotten': 'rotten',\n",
        "        'eggplant_healthy': 'ripe',\n",
        "        'eggplant_rotten': 'rotten',\n",
        "        'grapes_healthy': 'ripe',\n",
        "        'grapes_rotten': 'rotten',\n",
        "        'spinach_healthy': 'ripe',\n",
        "        'spinach_rotten': 'rotten',\n",
        "        'tomato_healthy': 'ripe',\n",
        "        'tomato_rotten': 'rotten'\n",
        "    }\n",
        "    return class_mapping\n",
        "\n",
        "def get_source_datasets():\n",
        "    \"\"\"Define and return paths to all source datasets with their class names\"\"\"\n",
        "    source_datasets = [\n",
        "        {\n",
        "            'name': 'banana-2',\n",
        "            'train_images': 'banana-2/train/images',\n",
        "            'train_labels': 'banana-2/train/labels',\n",
        "            'val_images': 'banana-2/valid/images',\n",
        "            'val_labels': 'banana-2/valid/labels',\n",
        "            'class_names': ['ripe banana', 'riper banana', 'rotten banana', 'unripe banana']\n",
        "        },\n",
        "        {\n",
        "            'name': 'Orange_Project',\n",
        "            'train_images': 'Orange_Project/images/train',\n",
        "            'train_labels': 'Orange_Project/label/train',\n",
        "            'val_images': 'Orange_Project/images/val',\n",
        "            'val_labels': 'Orange_Project/label/val',\n",
        "            'class_names': ['Ripe', 'Unripe']\n",
        "        },\n",
        "        {\n",
        "            'name': 'orange_detection-2',\n",
        "            'train_images': 'orange_detection-2/train/images',\n",
        "            'train_labels': 'orange_detection-2/train/labels',\n",
        "            'val_images': 'orange_detection-2/valid/images',\n",
        "            'val_labels': 'orange_detection-2/valid/labels',\n",
        "            'class_names': ['RottenOranges', 'orange']\n",
        "        },\n",
        "        {\n",
        "            'name': 'tomato-checker-1',\n",
        "            'train_images': 'tomato-checker-1/train/images',\n",
        "            'train_labels': 'tomato-checker-1/train/labels',\n",
        "            'val_images': 'tomato-checker-1/valid/images',\n",
        "            'val_labels': 'tomato-checker-1/valid/labels',\n",
        "            'class_names': ['damaged', 'healthy ripe', 'unripe']\n",
        "        },\n",
        "        {\n",
        "            'name': 'rot_detection-13',\n",
        "            'train_images': 'rot_detection-13/train/images',\n",
        "            'train_labels': 'rot_detection-13/train/labels',\n",
        "            'val_images': 'rot_detection-13/valid/images',\n",
        "            'val_labels': 'rot_detection-13/valid/labels',\n",
        "            'class_names': [\n",
        "                'cucumber_healthy', 'cucumber_rotten',\n",
        "                'eggplant_healthy', 'eggplant_rotten',\n",
        "                'grapes_healthy', 'grapes_rotten',\n",
        "                'spinach_healthy', 'spinach_rotten',\n",
        "                'tomato_healthy', 'tomato_rotten'\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    return source_datasets\n",
        "\n",
        "def update_label_class_ids(label_file, class_mapping_ids, original_class_names):\n",
        "    \"\"\"Update class IDs in the label files\"\"\"\n",
        "    with open(label_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    updated_lines = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if not parts:\n",
        "            continue\n",
        "\n",
        "        original_class_id = int(parts[0])\n",
        "        if original_class_id >= len(original_class_names):\n",
        "            print(f\"Warning: Class ID {original_class_id} out of range in {label_file}\")\n",
        "            continue\n",
        "\n",
        "        original_class_name = original_class_names[original_class_id]\n",
        "\n",
        "        if original_class_name in class_mapping_ids:\n",
        "            new_class_id = class_mapping_ids[original_class_name]\n",
        "            updated_line = f\"{new_class_id} {' '.join(parts[1:])}\\n\"\n",
        "            updated_lines.append(updated_line)\n",
        "\n",
        "    with open(label_file, 'w') as f:\n",
        "        f.writelines(updated_lines)\n",
        "\n",
        "def process_dataset():\n",
        "    \"\"\"Main function to process and combine all datasets\"\"\"\n",
        "    # Setup directories\n",
        "    output_dir = \"combined_dataset\"\n",
        "    train_images_dir = os.path.join(output_dir, \"train\", \"images\")\n",
        "    train_labels_dir = os.path.join(output_dir, \"train\", \"labels\")\n",
        "    val_images_dir = os.path.join(output_dir, \"val\", \"images\")\n",
        "    val_labels_dir = os.path.join(output_dir, \"val\", \"labels\")\n",
        "\n",
        "    # Create directories\n",
        "    create_directory(train_images_dir)\n",
        "    create_directory(train_labels_dir)\n",
        "    create_directory(val_images_dir)\n",
        "    create_directory(val_labels_dir)\n",
        "\n",
        "    # Load class mapping\n",
        "    class_mapping = parse_class_mapping()\n",
        "\n",
        "    # Create a list of all final classes and assign IDs\n",
        "    final_classes = sorted(set(v for v in class_mapping.values() if v is not None))\n",
        "    final_class_to_id = {class_name: idx for idx, class_name in enumerate(final_classes)}\n",
        "\n",
        "    # Count images per class to enforce maximum limit\n",
        "    class_image_count = {class_name: 0 for class_name in final_classes}\n",
        "    max_images_per_class = 3000\n",
        "\n",
        "    # Gather all images and labels from source datasets\n",
        "    all_images = []\n",
        "    source_datasets = get_source_datasets()\n",
        "\n",
        "    for dataset in source_datasets:\n",
        "        try:\n",
        "            # Get the original class names directly from the dataset info\n",
        "            original_class_names = dataset['class_names']\n",
        "\n",
        "            # Create a mapping from original class IDs to new class IDs\n",
        "            class_mapping_ids = {}\n",
        "            for idx, class_name in enumerate(original_class_names):\n",
        "                if class_name in class_mapping and class_mapping[class_name] is not None:\n",
        "                    target_class = class_mapping[class_name]\n",
        "                    class_mapping_ids[class_name] = final_class_to_id[target_class]\n",
        "\n",
        "            # Process training images and labels\n",
        "            train_images = glob.glob(os.path.join(dataset['train_images'], \"*.*\"))\n",
        "            for img_path in train_images:\n",
        "                img_filename = os.path.basename(img_path)\n",
        "                label_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
        "                label_path = os.path.join(dataset['train_labels'], label_filename)\n",
        "\n",
        "                # Skip if label file doesn't exist\n",
        "                if not os.path.exists(label_path):\n",
        "                    continue\n",
        "\n",
        "                # Determine the class from the label file\n",
        "                with open(label_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                if not lines:\n",
        "                    continue\n",
        "\n",
        "                line_parts = lines[0].strip().split()\n",
        "                if not line_parts:\n",
        "                    continue\n",
        "\n",
        "                original_class_id = int(line_parts[0])\n",
        "                if original_class_id >= len(original_class_names):\n",
        "                    continue\n",
        "\n",
        "                original_class_name = original_class_names[original_class_id]\n",
        "                if original_class_name not in class_mapping or class_mapping[original_class_name] is None:\n",
        "                    continue\n",
        "\n",
        "                target_class = class_mapping[original_class_name]\n",
        "\n",
        "                # Skip if we've reached the maximum for this class\n",
        "                if class_image_count[target_class] >= max_images_per_class:\n",
        "                    continue\n",
        "\n",
        "                # Add image and corresponding label to our collection\n",
        "                all_images.append({\n",
        "                    'dataset': dataset['name'],\n",
        "                    'img_path': img_path,\n",
        "                    'label_path': label_path,\n",
        "                    'class': target_class,\n",
        "                    'original_class_names': original_class_names,\n",
        "                    'split': 'train'\n",
        "                })\n",
        "\n",
        "                class_image_count[target_class] += 1\n",
        "\n",
        "            # Process validation images and labels\n",
        "            val_images = glob.glob(os.path.join(dataset['val_images'], \"*.*\"))\n",
        "            for img_path in val_images:\n",
        "                img_filename = os.path.basename(img_path)\n",
        "                label_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
        "                label_path = os.path.join(dataset['val_labels'], label_filename)\n",
        "\n",
        "                # Skip if label file doesn't exist\n",
        "                if not os.path.exists(label_path):\n",
        "                    continue\n",
        "\n",
        "                # Determine the class from the label file\n",
        "                with open(label_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                if not lines:\n",
        "                    continue\n",
        "\n",
        "                line_parts = lines[0].strip().split()\n",
        "                if not line_parts:\n",
        "                    continue\n",
        "\n",
        "                original_class_id = int(line_parts[0])\n",
        "                if original_class_id >= len(original_class_names):\n",
        "                    continue\n",
        "\n",
        "                original_class_name = original_class_names[original_class_id]\n",
        "                if original_class_name not in class_mapping or class_mapping[original_class_name] is None:\n",
        "                    continue\n",
        "\n",
        "                target_class = class_mapping[original_class_name]\n",
        "\n",
        "                # Skip if we've reached the maximum for this class\n",
        "                if class_image_count[target_class] >= max_images_per_class:\n",
        "                    continue\n",
        "\n",
        "                # Add image and corresponding label to our collection\n",
        "                all_images.append({\n",
        "                    'dataset': dataset['name'],\n",
        "                    'img_path': img_path,\n",
        "                    'label_path': label_path,\n",
        "                    'class': target_class,\n",
        "                    'original_class_names': original_class_names,\n",
        "                    'split': 'val'\n",
        "                })\n",
        "\n",
        "                class_image_count[target_class] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing dataset {dataset['name']}: {e}\")\n",
        "\n",
        "    # Print class distribution before splitting\n",
        "    print(\"Class distribution before re-splitting:\")\n",
        "    for class_name, count in class_image_count.items():\n",
        "        print(f\"  {class_name}: {count} images\")\n",
        "\n",
        "    # Shuffle all images for better distribution\n",
        "    random.shuffle(all_images)\n",
        "\n",
        "    # Create new 70:30 split\n",
        "    class_train_count = {class_name: 0 for class_name in final_classes}\n",
        "    class_val_count = {class_name: 0 for class_name in final_classes}\n",
        "\n",
        "    # Reset class counts for the new split\n",
        "    for img_info in all_images:\n",
        "        class_name = img_info['class']\n",
        "        total_class_count = class_image_count[class_name]\n",
        "        train_target = int(total_class_count * 0.7)  # 70% for training\n",
        "\n",
        "        # Assign to train or val based on current counts\n",
        "        if class_train_count[class_name] < train_target:\n",
        "            img_info['new_split'] = 'train'\n",
        "            class_train_count[class_name] += 1\n",
        "        else:\n",
        "            img_info['new_split'] = 'val'\n",
        "            class_val_count[class_name] += 1\n",
        "\n",
        "    # Print final split statistics\n",
        "    print(\"\\nFinal split statistics:\")\n",
        "    print(\"Train:\")\n",
        "    for class_name, count in class_train_count.items():\n",
        "        print(f\"  {class_name}: {count} images\")\n",
        "\n",
        "    print(\"\\nValidation:\")\n",
        "    for class_name, count in class_val_count.items():\n",
        "        print(f\"  {class_name}: {count} images\")\n",
        "\n",
        "    # Copy images and labels to new directories\n",
        "    for img_info in all_images:\n",
        "        try:\n",
        "            img_filename = os.path.basename(img_info['img_path'])\n",
        "            dataset_prefix = re.sub(r'[^a-zA-Z0-9]', '_', img_info['dataset'])\n",
        "\n",
        "            # Generate unique filenames by prefixing with dataset name\n",
        "            unique_img_filename = f\"{dataset_prefix}_{img_filename}\"\n",
        "            label_filename = os.path.splitext(unique_img_filename)[0] + \".txt\"\n",
        "\n",
        "            # Determine destination directories\n",
        "            if img_info['new_split'] == 'train':\n",
        "                dest_img_dir = train_images_dir\n",
        "                dest_label_dir = train_labels_dir\n",
        "            else:\n",
        "                dest_img_dir = val_images_dir\n",
        "                dest_label_dir = val_labels_dir\n",
        "\n",
        "            # Copy image file\n",
        "            shutil.copy2(img_info['img_path'], os.path.join(dest_img_dir, unique_img_filename))\n",
        "\n",
        "            # Copy and update label file\n",
        "            temp_label_path = os.path.join(dest_label_dir, label_filename)\n",
        "            shutil.copy2(img_info['label_path'], temp_label_path)\n",
        "\n",
        "            # Update class IDs in the label file\n",
        "            class_mapping_ids = {}\n",
        "            for idx, class_name in enumerate(img_info['original_class_names']):\n",
        "                if class_name in class_mapping and class_mapping[class_name] is not None:\n",
        "                    target_class = class_mapping[class_name]\n",
        "                    class_mapping_ids[class_name] = final_class_to_id[target_class]\n",
        "\n",
        "            update_label_class_ids(temp_label_path, class_mapping_ids, img_info['original_class_names'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying file: {e}\")\n",
        "\n",
        "    # Create dataset.yaml file\n",
        "    dataset_yaml = {\n",
        "        'path': os.path.abspath(output_dir),\n",
        "        'train': 'train/images',\n",
        "        'val': 'val/images',\n",
        "        'names': {idx: name for name, idx in final_class_to_id.items()}\n",
        "    }\n",
        "\n",
        "    save_yaml(dataset_yaml, os.path.join(output_dir, \"dataset.yaml\"))\n",
        "    print(f\"\\nCombined dataset created at: {os.path.abspath(output_dir)}\")\n",
        "    print(f\"Dataset YAML file: {os.path.join(output_dir, 'dataset.yaml')}\")\n",
        "    print(f\"\\nClass ID mapping:\")\n",
        "    for class_name, class_id in final_class_to_id.items():\n",
        "        print(f\"  {class_id}: {class_name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lFNHWJ19lvq"
      },
      "source": [
        "# Model training & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "5267cf54-1525-4468-9e9a-2485374e4aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 44.0/112.6 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "APSzoB9L7BxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "9709617f-33c9-4fae-914f-786800978458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt to 'yolo11s.pt'...\n",
            "100% 18.4M/18.4M [00:00<00:00, 358MB/s]\n",
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/combined_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 169MB/s]\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
            " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
            " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 23        [16, 19, 22]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
            "YOLO11s summary: 181 layers, 9,428,953 parameters, 9,428,937 gradients, 21.6 GFLOPs\n",
            "\n",
            "Transferred 493/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 394MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 13.2±5.4 MB/s, size: 46.2 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/combined_dataset/train/labels... 5588 images, 0 backgrounds, 0 corrupt: 100% 5588/5588 [00:13<00:00, 410.32it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/combined_dataset/train/labels.cache\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 193, len(boxes) = 9000. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 13.3±5.4 MB/s, size: 43.7 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined_dataset/val/labels... 2396 images, 0 backgrounds, 0 corrupt: 100% 2396/2396 [00:05<00:00, 415.85it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/combined_dataset/val/labels.cache\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 95, len(boxes) = 3938. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20      4.09G      1.212      1.637      1.563          9        640: 100% 350/350 [02:04<00:00,  2.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.82it/s]\n",
            "                   all       2396       3938      0.558      0.454      0.466      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20       4.9G      1.262      1.414      1.577         15        640: 100% 350/350 [01:59<00:00,  2.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.88it/s]\n",
            "                   all       2396       3938      0.511      0.493      0.457      0.291\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20      4.95G      1.252      1.378      1.569         15        640: 100% 350/350 [01:56<00:00,  3.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.96it/s]\n",
            "                   all       2396       3938      0.626      0.481      0.508      0.341\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20         5G      1.181      1.271      1.517         14        640: 100% 350/350 [01:55<00:00,  3.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.96it/s]\n",
            "                   all       2396       3938       0.71       0.61      0.665      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/20      5.02G      1.159      1.207      1.486          7        640: 100% 350/350 [01:55<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.96it/s]\n",
            "                   all       2396       3938      0.691      0.616      0.683      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/20      5.08G      1.108      1.135      1.452         18        640: 100% 350/350 [01:54<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.87it/s]\n",
            "                   all       2396       3938      0.715      0.614      0.703      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/20       5.1G      1.089      1.086      1.435         10        640: 100% 350/350 [01:58<00:00,  2.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.88it/s]\n",
            "                   all       2396       3938      0.714      0.678      0.726      0.525\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/20      5.16G      1.069      1.038      1.414         19        640: 100% 350/350 [01:58<00:00,  2.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.90it/s]\n",
            "                   all       2396       3938      0.672      0.658      0.689       0.49\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/20      5.18G      1.039      1.007      1.397         10        640: 100% 350/350 [01:59<00:00,  2.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.80it/s]\n",
            "                   all       2396       3938      0.752      0.663      0.744      0.538\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/20      5.24G      1.012     0.9669      1.381          9        640: 100% 350/350 [01:57<00:00,  2.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.82it/s]\n",
            "                   all       2396       3938      0.711      0.693      0.733      0.525\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/20      5.26G       1.09     0.9713      1.504          9        640: 100% 350/350 [01:56<00:00,  3.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.83it/s]\n",
            "                   all       2396       3938      0.753      0.751      0.787      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/20      5.31G      1.086      0.936      1.496          4        640: 100% 350/350 [01:55<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.81it/s]\n",
            "                   all       2396       3938      0.739      0.735      0.783      0.566\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/20      5.34G       1.05     0.9036      1.465         20        640: 100% 350/350 [01:54<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.93it/s]\n",
            "                   all       2396       3938      0.807      0.726      0.796      0.588\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/20      5.39G      1.038     0.8674      1.455          4        640: 100% 350/350 [01:53<00:00,  3.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.88it/s]\n",
            "                   all       2396       3938      0.816      0.745      0.814      0.607\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/20      5.41G      1.015     0.8226      1.428          4        640: 100% 350/350 [01:54<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.84it/s]\n",
            "                   all       2396       3938      0.802      0.761      0.819      0.605\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/20      5.47G     0.9943     0.7936      1.409          4        640: 100% 350/350 [01:56<00:00,  3.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.81it/s]\n",
            "                   all       2396       3938      0.788      0.768      0.811      0.607\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/20      5.49G     0.9811     0.7722      1.394          5        640: 100% 350/350 [01:55<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.84it/s]\n",
            "                   all       2396       3938      0.803       0.77       0.83      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/20      5.55G     0.9689      0.741      1.376          5        640: 100% 350/350 [01:54<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.85it/s]\n",
            "                   all       2396       3938      0.825      0.788      0.839       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/20      5.57G     0.9477     0.7178      1.359          4        640: 100% 350/350 [01:54<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:26<00:00,  2.87it/s]\n",
            "                   all       2396       3938      0.809      0.803      0.847      0.641\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20      5.63G     0.9275     0.6819      1.349          6        640: 100% 350/350 [01:54<00:00,  3.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:25<00:00,  2.89it/s]\n",
            "                   all       2396       3938      0.823      0.802      0.847       0.65\n",
            "\n",
            "20 epochs completed in 0.797 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 19.2MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 19.2MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11s summary (fused): 100 layers, 9,413,961 parameters, 0 gradients, 21.3 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 75/75 [00:28<00:00,  2.60it/s]\n",
            "                   all       2396       3938      0.822      0.802      0.847       0.65\n",
            "                  ripe        900       1077      0.896      0.949      0.972      0.895\n",
            "                rotten        864       1810      0.703       0.53       0.62      0.332\n",
            "                unripe        632       1051      0.868      0.928      0.947      0.721\n",
            "Speed: 0.2ms preprocess, 5.1ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "# Train YOLO11s on combined dataset for 20 epochs\n",
        "!yolo train model=yolo11s.pt data=/content/combined_dataset/dataset.yaml epochs=20 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model and Results"
      ],
      "metadata": {
        "id": "bZRjXxbY7iaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export model in ONNX format( for ease of deployment)"
      ],
      "metadata": {
        "id": "hdtisaHVP8SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=/content/runs/detect/train/weights/best.pt format=onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiIddvN2P7aK",
        "outputId": "790a6e70-92d2-4d01-a7b2-54c137db7b26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "💡 ProTip: Export to OpenVINO format for best performance on Intel CPUs. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
            "YOLO11s summary (fused): 100 layers, 9,413,961 parameters, 0 gradients, 21.3 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/runs/detect/train/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 7, 8400) (18.3 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0,<1.18.0', 'onnxslim>=0.1.46', 'onnxruntime'] not found, attempting AutoUpdate...\n",
            "Collecting onnx<1.18.0,>=1.12.0\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxslim>=0.1.46\n",
            "  Downloading onnxslim-0.1.53-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx<1.18.0,>=1.12.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx<1.18.0,>=1.12.0) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxslim>=0.1.46) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxslim>=0.1.46) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxslim>=0.1.46) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 125.0 MB/s eta 0:00:00\n",
            "Downloading onnxslim-0.1.53-py3-none-any.whl (146 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.2/146.2 kB 345.6 MB/s eta 0:00:00\n",
            "Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.4/16.4 MB 171.2 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 302.4 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 300.4 MB/s eta 0:00:00\n",
            "Installing collected packages: onnx, humanfriendly, onnxslim, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.22.0 onnxslim-0.1.53\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 7.7s, installed 3 packages: ['onnx>=1.12.0,<1.18.0', 'onnxslim>=0.1.46', 'onnxruntime']\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 10.3s, saved as '/content/runs/detect/train/weights/best.onnx' (36.2 MB)\n",
            "\n",
            "Export complete (12.6s)\n",
            "Results saved to \u001b[1m/content/runs/detect/train/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/content/runs/detect/train/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=/content/runs/detect/train/weights/best.onnx imgsz=640 data=/content/combined_dataset/dataset.yaml  \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Model Training results"
      ],
      "metadata": {
        "id": "ZpKgw7PJ5jJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "folder_path = '/content/runs/detect/train'\n",
        "zip_name = 'run_plots.zip'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(\"run_plots\", 'zip', folder_path)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "jZ1DYMK65hoh",
        "outputId": "492c7403-4468-437e-cdfb-a53072c9d32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2fc106f2-44d7-4a81-8fc7-08a24d94b8ca\", \"run_plots.zip\", 42269711)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}